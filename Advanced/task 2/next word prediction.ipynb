{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1027348f",
   "metadata": {},
   "source": [
    "# LGM-VIP Data Science Internship Programme\n",
    "    \n",
    " ### Advanced Level Task-2\n",
    " ### Next Word Prediction\n",
    "      Using Tensorflow and Keras library train a RNN, to predict the next word. \n",
    "      Dataset Link: https://drive.google.com/file/d/1GeUzNVqiixXHnTl8oNiQ2W3CynX_lsu2/view\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7addff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import heapq\n",
    "import warnings as wg\n",
    "wg.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f631ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source text\n",
    "file = open('1661-0.txt', encoding=\"utf8\").read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a546709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 581888\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14faf902",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"1661-0.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "#print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f377e011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle  This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.  You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.net   Title: The Adventures of Sherl\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216c4d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Project Gutenberg s The Adventures of Sherlock Holmes  by Arthur Conan Doyle  This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever   You may copy it  give it away or re use it under the terms of the Project Gutenberg License included with this eBook or online at www gutenberg net   Title  The Adventures of Sherlock Holmes  Author  Arthur Conan Doyle  Release Date  November 29  2002  EBook  1661  Last Updated  May 20  2019  Language  English  Charact'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ccca9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use anyone anywhere at no cost and with almost restrictions whatsoever. You may copy it, give it away or re-use under terms Gutenberg License included this online www.gutenberg.net Title: Holmes Author: Release Date: November 29, 2002 [EBook #1661] Last Updated: May 20, 2019 Language: English Character set encoding: UTF-8 *** START OF THIS PROJECT GUTENBERG EBOOK THE ADVENTURES SHERLOCK HOLMES Prod\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97c1a7",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8017ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[838, 3083, 56, 322, 57, 1523, 15, 95, 839, 3084]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ebf6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d58bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  17678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 838, 3083],\n",
       "       [3083,   56],\n",
       "       [  56,  322],\n",
       "       [ 322,   57],\n",
       "       [  57, 1523],\n",
       "       [1523,   15],\n",
       "       [  15,   95],\n",
       "       [  95,  839],\n",
       "       [ 839, 3084],\n",
       "       [3084, 3085]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "197fea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 838 3083   56  322   57]\n",
      "The responses are:  [3083   56  322   57 1523]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6f230b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e62cb",
   "metadata": {},
   "source": [
    "## RNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be39a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80ab0182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             89310     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8931)              8939931   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,078,241\n",
      "Trainable params: 22,078,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40f83f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## call back\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97eda942",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d4f9fc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.4565\n",
      "Epoch 00001: loss improved from 2.67389 to 2.45648, saving model to nextword1.h5\n",
      "118/118 [==============================] - 99s 835ms/step - loss: 2.4565 - lr: 0.0010\n",
      "Epoch 2/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.2583\n",
      "Epoch 00002: loss improved from 2.45648 to 2.25833, saving model to nextword1.h5\n",
      "118/118 [==============================] - 101s 857ms/step - loss: 2.2583 - lr: 0.0010\n",
      "Epoch 3/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.1978\n",
      "Epoch 00004: loss improved from 2.22229 to 2.19776, saving model to nextword1.h5\n",
      "118/118 [==============================] - 82s 699ms/step - loss: 2.1978 - lr: 0.0010\n",
      "Epoch 5/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.1789\n",
      "Epoch 00005: loss improved from 2.19776 to 2.17893, saving model to nextword1.h5\n",
      "118/118 [==============================] - 85s 717ms/step - loss: 2.1789 - lr: 0.0010\n",
      "Epoch 6/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.1617\n",
      "Epoch 00006: loss improved from 2.17893 to 2.16172, saving model to nextword1.h5\n",
      "118/118 [==============================] - 83s 705ms/step - loss: 2.1617 - lr: 0.0010\n",
      "Epoch 7/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.1251\n",
      "Epoch 00008: loss improved from 2.13843 to 2.12511, saving model to nextword1.h5\n",
      "118/118 [==============================] - 68s 576ms/step - loss: 2.1251 - lr: 0.0010\n",
      "Epoch 9/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.1150\n",
      "Epoch 00009: loss improved from 2.12511 to 2.11502, saving model to nextword1.h5\n",
      "118/118 [==============================] - 61s 515ms/step - loss: 2.1150 - lr: 0.0010\n",
      "Epoch 10/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0971\n",
      "Epoch 00010: loss improved from 2.11502 to 2.09706, saving model to nextword1.h5\n",
      "118/118 [==============================] - 66s 563ms/step - loss: 2.0971 - lr: 0.0010\n",
      "Epoch 11/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0791\n",
      "Epoch 00011: loss improved from 2.09706 to 2.07907, saving model to nextword1.h5\n",
      "118/118 [==============================] - 63s 540ms/step - loss: 2.0791 - lr: 0.0010\n",
      "Epoch 12/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0713\n",
      "Epoch 00012: loss improved from 2.07907 to 2.07133, saving model to nextword1.h5\n",
      "118/118 [==============================] - 62s 530ms/step - loss: 2.0713 - lr: 0.0010\n",
      "Epoch 13/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0626\n",
      "Epoch 00013: loss improved from 2.07133 to 2.06256, saving model to nextword1.h5\n",
      "118/118 [==============================] - 69s 584ms/step - loss: 2.0626 - lr: 0.0010\n",
      "Epoch 14/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0521\n",
      "Epoch 00014: loss improved from 2.06256 to 2.05214, saving model to nextword1.h5\n",
      "118/118 [==============================] - 76s 645ms/step - loss: 2.0521 - lr: 0.0010\n",
      "Epoch 15/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0364\n",
      "Epoch 00015: loss improved from 2.05214 to 2.03638, saving model to nextword1.h5\n",
      "118/118 [==============================] - 67s 569ms/step - loss: 2.0364 - lr: 0.0010\n",
      "Epoch 16/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0194\n",
      "Epoch 00016: loss improved from 2.03638 to 2.01935, saving model to nextword1.h5\n",
      "118/118 [==============================] - 59s 500ms/step - loss: 2.0194 - lr: 0.0010\n",
      "Epoch 17/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0206\n",
      "Epoch 00017: loss did not improve from 2.01935\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 2.0206 - lr: 0.0010\n",
      "Epoch 18/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0088\n",
      "Epoch 00018: loss improved from 2.01935 to 2.00877, saving model to nextword1.h5\n",
      "118/118 [==============================] - 64s 541ms/step - loss: 2.0088 - lr: 0.0010\n",
      "Epoch 19/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 2.0016\n",
      "Epoch 00019: loss improved from 2.00877 to 2.00163, saving model to nextword1.h5\n",
      "118/118 [==============================] - 69s 587ms/step - loss: 2.0016 - lr: 0.0010\n",
      "Epoch 20/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.9955\n",
      "Epoch 00020: loss improved from 2.00163 to 1.99548, saving model to nextword1.h5\n",
      "118/118 [==============================] - 85s 724ms/step - loss: 1.9955 - lr: 0.0010\n",
      "Epoch 21/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.9943\n",
      "Epoch 00021: loss improved from 1.99548 to 1.99430, saving model to nextword1.h5\n",
      "118/118 [==============================] - 97s 819ms/step - loss: 1.9943 - lr: 0.0010\n",
      "Epoch 22/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.9899\n",
      "Epoch 00022: loss improved from 1.99430 to 1.98988, saving model to nextword1.h5\n",
      "118/118 [==============================] - 76s 647ms/step - loss: 1.9899 - lr: 0.0010\n",
      "Epoch 23/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.9832\n",
      "Epoch 00023: loss improved from 1.98988 to 1.98325, saving model to nextword1.h5\n",
      "118/118 [==============================] - 76s 650ms/step - loss: 1.9832 - lr: 0.0010\n",
      "Epoch 24/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.9723\n",
      "Epoch 00024: loss improved from 1.98325 to 1.97227, saving model to nextword1.h5\n",
      "118/118 [==============================] - 76s 640ms/step - loss: 1.9723 - lr: 0.0010\n",
      "Epoch 25/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.9504\n",
      "Epoch 00026: loss improved from 1.96958 to 1.95042, saving model to nextword1.h5\n",
      "118/118 [==============================] - 67s 567ms/step - loss: 1.9504 - lr: 0.0010\n",
      "Epoch 27/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.9424\n",
      "Epoch 00027: loss improved from 1.95042 to 1.94236, saving model to nextword1.h5\n",
      "118/118 [==============================] - 66s 561ms/step - loss: 1.9424 - lr: 0.0010\n",
      "Epoch 28/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.9199\n",
      "Epoch 00028: loss improved from 1.94236 to 1.91988, saving model to nextword1.h5\n",
      "118/118 [==============================] - 68s 574ms/step - loss: 1.9199 - lr: 0.0010\n",
      "Epoch 29/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8963\n",
      "Epoch 00030: loss improved from 1.91171 to 1.89634, saving model to nextword1.h5\n",
      "118/118 [==============================] - 70s 596ms/step - loss: 1.8963 - lr: 0.0010\n",
      "Epoch 31/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8790\n",
      "Epoch 00031: loss improved from 1.89634 to 1.87900, saving model to nextword1.h5\n",
      "118/118 [==============================] - 68s 572ms/step - loss: 1.8790 - lr: 0.0010\n",
      "Epoch 32/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8703\n",
      "Epoch 00032: loss improved from 1.87900 to 1.87032, saving model to nextword1.h5\n",
      "118/118 [==============================] - 72s 613ms/step - loss: 1.8703 - lr: 0.0010\n",
      "Epoch 33/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8595\n",
      "Epoch 00033: loss improved from 1.87032 to 1.85952, saving model to nextword1.h5\n",
      "118/118 [==============================] - 71s 604ms/step - loss: 1.8595 - lr: 0.0010\n",
      "Epoch 34/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8514\n",
      "Epoch 00035: loss improved from 1.85802 to 1.85137, saving model to nextword1.h5\n",
      "118/118 [==============================] - 66s 560ms/step - loss: 1.8514 - lr: 0.0010\n",
      "Epoch 36/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8441\n",
      "Epoch 00036: loss improved from 1.85137 to 1.84411, saving model to nextword1.h5\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 1.8441 - lr: 0.0010\n",
      "Epoch 37/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8409\n",
      "Epoch 00037: loss improved from 1.84411 to 1.84093, saving model to nextword1.h5\n",
      "118/118 [==============================] - 56s 477ms/step - loss: 1.8409 - lr: 0.0010\n",
      "Epoch 38/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8282\n",
      "Epoch 00039: loss improved from 1.83926 to 1.82823, saving model to nextword1.h5\n",
      "118/118 [==============================] - 75s 635ms/step - loss: 1.8282 - lr: 0.0010\n",
      "Epoch 40/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8268\n",
      "Epoch 00040: loss improved from 1.82823 to 1.82676, saving model to nextword1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 62s 524ms/step - loss: 1.8268 - lr: 0.0010\n",
      "Epoch 41/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8200\n",
      "Epoch 00041: loss improved from 1.82676 to 1.81998, saving model to nextword1.h5\n",
      "118/118 [==============================] - 57s 487ms/step - loss: 1.8200 - lr: 0.0010\n",
      "Epoch 42/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8211\n",
      "Epoch 00043: loss did not improve from 1.81998\n",
      "118/118 [==============================] - 57s 482ms/step - loss: 1.8211 - lr: 0.0010\n",
      "Epoch 44/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8157\n",
      "Epoch 00044: loss improved from 1.81998 to 1.81565, saving model to nextword1.h5\n",
      "118/118 [==============================] - 53s 451ms/step - loss: 1.8157 - lr: 0.0010\n",
      "Epoch 45/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8110\n",
      "Epoch 00045: loss improved from 1.81565 to 1.81100, saving model to nextword1.h5\n",
      "118/118 [==============================] - 53s 453ms/step - loss: 1.8110 - lr: 0.0010\n",
      "Epoch 46/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8155\n",
      "Epoch 00046: loss did not improve from 1.81100\n",
      "118/118 [==============================] - 52s 446ms/step - loss: 1.8155 - lr: 0.0010\n",
      "Epoch 47/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8152\n",
      "Epoch 00047: loss did not improve from 1.81100\n",
      "118/118 [==============================] - 53s 452ms/step - loss: 1.8152 - lr: 0.0010\n",
      "Epoch 48/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8031\n",
      "Epoch 00048: loss improved from 1.81100 to 1.80313, saving model to nextword1.h5\n",
      "118/118 [==============================] - 53s 454ms/step - loss: 1.8031 - lr: 0.0010\n",
      "Epoch 49/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.8037\n",
      "Epoch 00049: loss did not improve from 1.80313\n",
      "118/118 [==============================] - 52s 439ms/step - loss: 1.8037 - lr: 0.0010\n",
      "Epoch 50/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.7846\n",
      "Epoch 00051: loss improved from 1.79687 to 1.78464, saving model to nextword1.h5\n",
      "118/118 [==============================] - 55s 470ms/step - loss: 1.7846 - lr: 0.0010\n",
      "Epoch 52/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.7795\n",
      "Epoch 00052: loss improved from 1.78464 to 1.77949, saving model to nextword1.h5\n",
      "118/118 [==============================] - 55s 464ms/step - loss: 1.7795 - lr: 0.0010\n",
      "Epoch 53/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.7654\n",
      "Epoch 00053: loss improved from 1.77949 to 1.76537, saving model to nextword1.h5\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 1.7654 - lr: 0.0010\n",
      "Epoch 54/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.7540\n",
      "Epoch 00054: loss improved from 1.76537 to 1.75405, saving model to nextword1.h5\n",
      "118/118 [==============================] - 58s 490ms/step - loss: 1.7540 - lr: 0.0010\n",
      "Epoch 55/55\n",
      "118/118 [==============================] - ETA: 0s - loss: 1.7488\n",
      "Epoch 00055: loss improved from 1.75405 to 1.74882, saving model to nextword1.h5\n",
      "118/118 [==============================] - 57s 487ms/step - loss: 1.7488 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26d8a006100>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=55, batch_size=150, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c743bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('nextword1.h5')\n",
    "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "  \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "  \n",
    "  print(predicted_word)\n",
    "  return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: loss improved\n",
      "['loss', 'improved']\n",
      "insinuating\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "  \n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break\n",
    "  \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "        \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "          \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826c291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
